# -*- coding: utf-8 -*-
"""Stroke Prediction Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TB9dVqxKpTwKGCY78_t1P73wEvigAKFd

# 🧠 Stroke Prediction Dataset Overview

## 📋 Dataset Information

*Total Rows:* 5110  
*Total Columns:* 12  

| Column Name       | Description |
|-------------------|-------------|
| id                | Unique identifier for each patient |
| gender            | Gender of the patient (Male, Female, Other) |
| age               | Age of the patient (in years) |
| hypertension      | 1 if the patient has hypertension, 0 otherwise |
| heart_disease     | 1 if the patient has any heart disease, 0 otherwise |
| ever_married      | Whether the patient was ever married (Yes/No) |
| work_type         | Type of work the person does (Private, Self-employed, Govt_job, etc.) |
| Residence_type    | Living area of the patient (Urban or Rural) |
| avg_glucose_level | Average glucose level in blood |
| bmi               | Body Mass Index (may contain missing values) |
| smoking_status    | Smoking habits of the patient (never smoked, smokes, formerly smoked, Unknown) |
| stroke            | *Target column* – 1 if the patient had a stroke, 0 otherwise |

---

## 🎯 Problem Statement

The goal of this dataset is to *predict whether a patient is likely to have a stroke* based on their health conditions and lifestyle attributes.  
This is a *binary classification problem* — the model predicts *1 (stroke)* or *0 (no stroke)*.

---

## 🧩 Why This Dataset?

This dataset is important because:

- 🩺 *Stroke* is one of the major causes of death worldwide.  
- ⚕ *Early prediction* can help doctors and individuals take preventive measures.  
- 🔬 It combines *demographic, medical, and lifestyle* factors — ideal for exploring *feature importance* and *correlations*.

---

## 🎯 Target Column

*Column Name:* stroke  
*Type:* Binary (0 or 1)  
*Purpose:* To identify whether the patient had a stroke.

#📦 **Step 1: Importing Required Libraries**

*Before starting data analysis or model building, we need to import essential Python libraries that help us handle data, visualize patterns, and perform computations efficiently.*
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""# **🔹 pandas**

**Purpose**: Used for data manipulation and analysis.

**Why we use it:**

Helps read and load datasets (like CSV files).

Provides powerful data structures such as DataFrame and Series to work with tabular data.

Makes it easy to clean, filter, and analyze data.

**Example usage**: pd.read_csv('data.csv') loads a dataset into a DataFrame for analysis.

# **🔹 matplotlib.pyplot**

**Purpose**: A popular data visualization library for creating static, animated, and interactive plots.

**Why we use it:**

To create bar charts, histograms, scatter plots, and line charts.

Helps us visually understand patterns, trends, and outliers in the data.

**Example usage** : plt.plot(x, y) or plt.hist(data) to visualize data distributions.

# **🔹 seaborn**

**Purpose** : A library built on top of Matplotlib that provides more attractive and informative statistical graphics.

**Why we use it:**

For advanced visualizations such as heatmaps, boxplots, and pair plots.

Makes visual analysis faster and more elegant with simple syntax.

**Example usage**: sns.heatmap(df.corr()) to visualize correlation between features.

# **🔹 numpy**

**Purpose**: Used for numerical computations and handling arrays or matrices.

**Why we use it:**

Provides fast mathematical operations on large datasets.

Commonly used for data preprocessing, statistical calculations, and as a backend for pandas and scikit-learn.

**Example usage** : np.mean(df['age']) to find the average age of patients.

# **📂 Step 2: Loading the Dataset**
"""

df = pd.read_csv('/content/stroke.csv')

# Displaying the DataFrame to get a quick look at the dataset
df

"""# **🎯 Purpose**

This step is used to load the dataset into a pandas DataFrame so that it can be easily explored and analyzed.

# **🧠 Explanation**

pd.read_csv('/content/stroke.csv')
→ Reads the CSV (Comma Separated Values) file named stroke.csv and loads it into a pandas DataFrame.
→ A DataFrame is a 2D tabular data structure with labeled rows and columns, making it ideal for data analysis.

**df**
→ Displays the first few rows of the dataset by default in Jupyter/Colab notebooks, giving a quick overview of the data’s structure and contents.

📊 **Output**

The output shows a table-like view of the dataset including all columns such as
id, gender, age, hypertension, heart_disease, etc.
This helps in getting a first impression of what kind of data we are dealing with.

# **👀 Step 3: Viewing the First Few Rows of the Dataset**
"""

df.head()

"""# **Step 4: Viewing the Last Few Rows of the Dataset**"""

df.tail()

"""# **🎯 Purpose**

The function df.tail() is used to display the last 5 rows of the dataset.
It helps to verify the ending portion of the data and check if the dataset is complete or contains missing/inconsistent entries at the bottom. *italicized text*

# **🧠 Explanation**

**df.tail()**
→ Shows the last 5 rows by default.
→ Useful for detecting issues like incomplete rows, extra headers, or unwanted blank entries at the end.
→ You can specify the number of rows you want to see: *italicized text*

df.tail(10)


This will show the last 10 rows instead of 5.

**📊 Output**

The output will display the final few rows of the dataset, showing the same columns as before — such as
id, gender, age, bmi, avg_glucose_level, stroke, etc.

# **🧩 Step 5: Checking Dataset Information**
"""

df.info()

"""# **🎯 Purpose**

The df.info() function is used to get a concise summary of the dataset.
It provides important information about the structure and data types of each column in your DataFrame.

# **🧠 Explanation**

When you run df.info(), it displays:

Total number of rows and columns in the dataset.

Column names (features of the dataset).

Non-null count → how many values are not missing in each column.

Data types (dtype) of each column — like int64, float64, object, etc.

Memory usage → how much space your DataFrame occupies in memory.

This helps you quickly understand the dataset’s structure, missing data, and variable types before applying preprocessing or models.

# **🧩 Step 6: Checking Data Types of Each Column**
"""

df.dtypes

"""# **🎯 Purpose**

The df.dtypes command is used to check the data type of each column in the dataset.
It helps you understand whether the data is numerical, categorical, or textual, which is crucial before data preprocessing and model building.

# **🧠 Explanation**

Every column in a DataFrame has a specific data type (dtype) that defines the kind of values it stores:

int64 → Integer values (whole numbers)

float64 → Decimal numbers

object → Text or categorical data (like gender, work type, etc.)

bool → True/False values

datetime64 → Date and time values *italicized text*

By knowing these types, you can decide:

Which columns need encoding (for categorical data),

Which columns need scaling or normalization (for numerical data),

And where type conversion might be needed.

# **📊 Step 7: Descriptive Statistics of the Dataset**
"""

df.describe()

"""# **🎯 Purpose**

The **df.describe()** function provides a statistical summary of the numerical columns in the dataset.
It helps to quickly understand the distribution, central tendency, and spread of the data.

# **🧠 Explanation**

This method automatically calculates and displays key statistical measures for all numeric columns, such as:

Metric	Meaning


count	Number of non-missing (non-null) values


mean	Average value of the column

std	Standard deviation — shows how spread out the data is

min	Minimum value in the column

25% (Q1)	1st quartile — 25% of data lies below this value

50% (Q2)	Median — middle value of the data

75% (Q3)	3rd quartile — 75% of data lies below this value *italicised text*


max	Maximum value in the column

# **🧾 Step 8: Displaying All Column Names**
"""

df.columns

"""# **🎯 Purpose**
The **df.columns** attribute is used to display the names of all columns present in the dataset.
It helps us quickly review the features (independent variables) and the target column available for analysis or modeling.

# **🧠 Explanation**

Every dataset has columns (features) that represent different aspects of the data.

df.columns returns an Index object containing all column names in the DataFrame.

**It is useful for:**

**Checking for typos or unwanted spaces in column names.**

**Renaming columns when needed.**

**Understanding the structure of the dataset before performing further operations.**

# **📐 Step 9: Checking the Shape of the Dataset**
"""

df.shape

"""# **🎯 Purpose**

The **df.shape **attribute is used to find out the dimensions of the dataset — that is, how many rows and columns it contains.
It gives a quick overview of the size of the DataFrame.

# **🧠 Explanation**

**df.shape** returns a tuple (rows, columns).

The first value represents the number of rows (records).

The second value represents the number of columns (features).

This helps to understand:

How large the dataset is.

Whether it’s suitable for analysis or machine learning.

If data was loaded completely (no missing rows or broken import).

# **🧩 Step 10: Checking for Duplicate Rows in the Dataset**
"""

df.duplicated().sum()

"""# **🎯 Purpose**

The function df.duplicated().sum() is used to identify and count duplicate rows in the dataset.


Duplicate data can cause bias, inaccurate analysis, and misleading model results, so detecting and handling them is an essential data-cleaning step.

# **🧠 Explanation**

df.duplicated() returns a Boolean Series where:

True → indicates that the row is a duplicate of a previous one.

False → means the row is unique.

sum() then counts how many True values exist — i.e., how many duplicate rows are in the dataset.

# **🧮 Step 11: Checking for Missing Values in the Dataset**
"""

df.isnull().sum()

"""# **🎯 Purpose**

The df.isnull().sum() function is used to detect and count missing (null or NaN) values in each column of the dataset.


Identifying missing data is one of the most important steps in data cleaning, as it directly affects model performance and accuracy.

# **🧠 Explanation**

df.isnull() returns a Boolean DataFrame:

True → indicates that the value is missing.

False → indicates that the value is present.

By applying .sum(), we count the total number of True values (i.e., missing entries) in each column.

# **🧮 Step 12: Handling Missing Values in the Dataset**
"""

df['bmi'].fillna(df['bmi'].mean(), inplace=True)

df.isnull().sum()

"""# **🎯 Purpose**

This command is used to fill (replace) the missing values in the column bmi with the mean (average) value of that same column.


It ensures there are no null values left, making the dataset clean and ready for further analysis or modeling.

# **🧠 Explanation**

Let’s break down the command:


Part	Meaning

**df['bmi']	Selects the bmi column from the dataset**

**.fillna()	A function used to replace NaN (missing) values**

**df['bmi'].mean()	Calculates the mean (average) of all existing bmi values**

**inplace=True	Updates the DataFrame directly without creating a new one**


So, this line finds the average BMI value and replaces every missing entry (NaN) in the bmi column with that average.

# **❤️ Step 13: Checking Target Variable Distribution**
"""

df['stroke'].value_counts()

sns.set()

"""# **🎯 Purpose**

The function df['stroke'].value_counts() is used to count the number of occurrences of each unique value in the stroke column — which is our target variable.


It helps us understand how balanced or imbalanced the dataset is for classification.

# **🧠 Explanation**

Part	Meaning

df['stroke']	Selects the target column (stroke) from the DataFrame.

.value_counts()	Counts how many times each unique value (0 or 1) appears in that column.


Since stroke is a binary column:

0 → means the person did not have a stroke.

1 → means the person had a stroke.

# **📊 Step 14: Visualizing Stroke Case Distribution**
"""

plt.figure(figsize=(5,4))
sns.countplot(x='stroke', data=df)
plt.title('Distribution of Stroke Cases')
plt.xlabel('Stroke (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.show()

sns.countplot(x='hypertension', data=df)

sns.countplot(x='heart_disease', data=df)

sns.countplot(x='ever_married', data=df)

sns.countplot(x='work_type', data=df)

sns.countplot(x='Residence_type', data=df)

sns.countplot(x='smoking_status', data=df)

"""# **🎯 Purpose**

This block of code is used to visualize the distribution of the target variable stroke using a count plot.
It helps to clearly see how many people had a stroke (1) vs how many did not (0).

# **🧠 Explanation**


Line	Description


plt.figure(figsize=(5,4))	Defines the size of the plot (width = 5, height = 4).
sns.countplot(x='stroke', data=df)	Uses Seaborn’s countplot to show the count of each category (0 and 1) in the stroke column.


plt.title('Distribution of Stroke Cases')	Adds a title to the chart for clarity.
plt.xlabel('Stroke (0 = No, 1 = Yes)')	Labels the x-axis to explain what 0 and 1 mean.
plt.ylabel('Count')	Labels the y-axis to indicate the number of records in each class.
plt.show()	Displays the final visualization.

# **📉 Step 22: Visualizing Feature Relationships Using Pairplot**
"""

pair_cols = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease', 'stroke']
# for speed, sample if the dataset is large
sns.pairplot(df[pair_cols].sample(frac=0.25, random_state=42), hue='stroke', corner=True)
plt.suptitle("Pairplot (sampled 25%)", y=1.02)
plt.show()

"""# **🧠 Explanation**

pair_cols → Selects important numeric and categorical features.


sample(frac=0.25) → Uses 25% of data for faster plotting.


sns.pairplot() → Creates scatterplots and histograms for feature pairs.


hue='stroke' → Colors points by stroke status.


corner=True → Shows only the lower triangle for clarity.

# **🧰 Step 16: Importing LabelEncoder and SMOTE**
"""

from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

"""# **🎯 Purpose**

These two imports are used for data preprocessing and handling class imbalance — two essential steps before training a machine learning model.



**LabelEncoder → Converts categorical data into numeric values.**

**SMOTE → Balances the dataset by generating synthetic samples for the minority class.**

# **🧠 Explanation**

***Library / Function	Description***


LabelEncoder	Imported from sklearn.preprocessing, it is used to encode categorical (text) data into numerical form so that models can process it.


SMOTE	Imported from imblearn.over_sampling, it stands for Synthetic Minority Oversampling Technique — a method to fix class imbalance by creating synthetic samples for the minority class instead of just duplicating data.

# **🔢 Step 15: Encoding Categorical Variables**
"""

# Identify categorical columns
cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Initialize label encoder
le = LabelEncoder()

# Apply label encoding to each categorical column
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

"""# **📊 Step: Correlation Heatmap (After Label Encoding)**"""

corr = df.drop('stroke', axis=1).corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Feature Correlation Matrix (After Label Encoding)")
plt.show()

"""# **🎯 Purpose**

To examine how numeric and label-encoded features are correlated with each other.

# **🧠 Explanation**

df.drop('stroke', axis=1) → Removes the target column before computing correlations.

.corr() → Calculates pairwise correlation coefficients among features.

sns.heatmap() → Displays correlations visually with color intensity.

annot=True → Shows numeric correlation values in each cell.

fmt=".2f" → Formats the correlation values to two decimal places.

# **🧮 Step 17: Splitting Features and Target Variable**
"""

X = df.drop('stroke', axis=1)
y = df['stroke']

"""# **🎯 Purpose**

This step separates the dataset into independent features (X) and the dependent target variable (y).
It’s a key step before training a machine learning model, as the model learns relationships between inputs (X) and outputs (y).

# **🧠 Explanation**
Code	Description

df.drop('stroke', axis=1)	Removes the 'stroke' column from the dataset — all remaining columns become features (X).

axis=1	Specifies that we are dropping a column, not a row.

df['stroke']	Selects the target column (stroke), which contains values 0 (no stroke) and 1 (stroke).

So after this:

**X → Contains all input features such as age, gender, bmi, hypertension, etc.**

**y → Contains the output/target labels (0 or 1).**

# **⚖️ Step 18: Balancing the Dataset Using SMOTE**
"""

smote = SMOTE(random_state=42)

# Fit and resample
X_resampled, y_resampled = smote.fit_resample(X, y)

"""# **🎯 Purpose**

This step uses SMOTE (Synthetic Minority Oversampling Technique) to balance the target classes by generating synthetic samples for the minority class (stroke = 1).



It ensures that both classes — stroke and non-stroke — have equal representation, improving model fairness and accuracy.

# **🧠 Explanation**


Code	Description


SMOTE(random_state=42)	Initializes the SMOTE object. random_state=42 ensures reproducibility (same results every time).


fit_resample(X, y)	Fits the SMOTE algorithm on the original dataset and resamples it — creating new synthetic examples for the minority class.


X_resampled, y_resampled	The resulting balanced datasets:


- X_resampled → features after balancing
- y_resampled → target labels after balancing

# **📊 Step 19: Checking Class Distribution After SMOTE**
"""

y_resampled.value_counts()

"""# **🎯 Purpose**

The purpose of this command is to verify whether the dataset has been successfully balanced after applying SMOTE (Synthetic Minority Oversampling Technique).
It counts how many samples belong to each class (0 = No Stroke, 1 = Stroke) in the resampled target variable y_resampled.

# **🧠 Explanation**
Code	Description
y_resampled	The resampled target variable after applying SMOTE.
.value_counts()	Returns the count of each unique value in y_resampled.

This helps confirm whether the SMOTE algorithm has created equal samples for both classes.

# **🧩 Step 20: Importing train_test_split**
"""

from sklearn.model_selection import train_test_split

"""# **🎯 Purpose**

The train_test_split function is used to divide the dataset into two parts:

Training set → used to train the machine learning model.

Testing set → used to evaluate how well the model performs on unseen data.

This ensures that the model can generalize effectively and not just memorize the training data.

# **🧠 Explanation**

Component	Description

sklearn.model_selection	A module from the Scikit-learn library that provides utilities for model selection and evaluation.


train_test_split	A function within this module used to randomly split datasets into training and testing subsets.

# **🧩 Step 21: Splitting the Dataset into Training and Testing Sets**
"""

X_train, X_test, y_train, y_test = train_test_split(
    X_resampled,
    y_resampled,
    test_size=0.2,
    random_state=42,
    stratify=y_resampled)

"""# **🎯 Purpose**

The goal of this step is to split the balanced dataset into training and testing subsets.

This allows us to:

Train the model on one portion of data (training set).

Test and evaluate its performance on unseen data (testing set).

This ensures the model’s predictions are generalizable and not just memorized.

# **🧠 Explanation of Each Parameter**
Parameter	Description

X_resampled	The feature set after applying SMOTE (input variables).

y_resampled	The balanced target labels (0 = no stroke, 1 = stroke).

test_size=0.2	Allocates 20% of the data for testing and 80% for training.

random_state=42	Ensures consistent results every time you run the code (reproducibility).

stratify=y_resampled	Keeps the same class proportion in both training and testing sets — prevents imbalance after splitting.

# **📏 Step 22: Checking the Shape of Training and Testing Sets**
"""

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

"""# **🎯 Purpose**

The purpose of this step is to verify the size and structure of the training and testing datasets after splitting.


It ensures that the data has been correctly divided and that the number of samples in each set matches the specified split ratio (e.g., 80% training, 20% testing).

# **🧠 Explanation**
Code	Description

X_train.shape	Returns a tuple showing the number of rows (samples) and columns (features) in the training data.


X_test.shape	Returns a tuple showing the same for the testing data.
print()	Displays the shape of both sets in a clear, readable format.

# **🤖 Step 16: Importing Multiple Machine Learning Models and Evaluation Metrics**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""# **🎯 Purpose**

This block imports different classification algorithms and evaluation metrics from the scikit-learn library.

It allows us to train, compare, and evaluate various models to find out which one performs best for predicting stroke occurrence.

# **🧠 Explanation**
# **🔹 LogisticRegression**

From: sklearn.linear_model

Purpose: Performs binary classification using a linear decision boundary.

Use Case: Works well when features and target are linearly related.

Strength: Simple, fast, and interpretable (shows feature importance through coefficients).

# **🔹 DecisionTreeClassifier**

From: sklearn.tree

Purpose: Builds a tree-like model of decisions and outcomes.

Use Case: Handles non-linear data effectively.

Strength: Easy to visualize and interpret; no need for feature scaling.

Weakness: Can overfit on training data if not pruned properly.

# **🔹 RandomForestClassifier**

From: sklearn.ensemble

Purpose: An ensemble model combining multiple decision trees to improve accuracy.

Use Case: Ideal for reducing overfitting and improving generalization.

Strength: High accuracy and robustness against noise.

Weakness: Less interpretable than a single decision tree.

# **🔹 GradientBoostingClassifier**

From: sklearn.ensemble

Purpose: Uses boosting — builds trees sequentially, where each new tree corrects previous errors.

Use Case: Excellent for handling imbalanced and complex datasets.

Strength: High predictive power and efficiency.

Weakness: Can be slow to train and sensitive to hyperparameters.

# 8**🔹 SVC (Support Vector Classifier)**

From: sklearn.svm

Purpose: Creates a hyperplane that separates classes with the maximum margin.

Use Case: Works well for both linear and non-linear problems (using kernels).

Strength: Effective in high-dimensional spaces.

Weakness: Can be slow on large datasets and sensitive to feature scaling.

# **🔹 KNeighborsClassifier (KNN)**

From: sklearn.neighbors

Purpose: Classifies a data point based on the majority class of its nearest neighbors.

Use Case: Simple algorithm useful for pattern recognition problems.

Strength: No training phase — works well for small datasets.

Weakness: Slow prediction time and sensitive to irrelevant features.

# **🔹 Evaluation Metrics**
Metric	Description
accuracy_score	Measures overall correctness of the model’s predictions.
confusion_matrix	Shows true vs. predicted values — helps identify model errors.
classification_report	Provides detailed metrics such as precision, recall, F1-score for each class.



# **📊 Why Use Multiple Models**

To compare performance of different algorithms on the same dataset.

To choose the best model based on metrics like accuracy, recall, and F1-score.

Different algorithms capture different data patterns, so trying several helps ensure robustness.



# **📈 Conclusion**

This step sets up the foundation for model comparison and evaluation.


By importing multiple algorithms and metrics, we can efficiently identify which model — whether simple (Logistic Regression) or complex (Random Forest, Gradient Boosting) — gives the most accurate and reliable stroke predictions.

# **🤖 Step 17: Training and Evaluating Multiple Machine Learning Models**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train the best model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""This code is used to train multiple machine learning models on the training data and compare their accuracy on the test data.
It helps identify which model performs best for predicting stroke occurrences.



# **🧠 Explanation**


# **🔹 Defining Models Dictionary**
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier()
}


Here, a dictionary named models is created.



Each key is the model name (string), and each value is a model object from scikit-learn.



This structure allows easy looping and comparison of multiple models without rewriting code for each one.

# **🔹 Creating an Empty Dictionary for Results**


results = {}


This dictionary will store each model’s accuracy score after testing.

Example format:

{'Logistic Regression': 0.85, 'Random Forest': 0.92, ...}

# **🔹 Looping Through Models**

**Step-by-Step Explanation:**

Looping through the models:
for name, model in models.items()
Iterates through each model and its name from the models dictionary.

Training the model:
model.fit(X_train, y_train)
Trains the model using the training dataset (X_train, y_train).

Making predictions:
y_pred = model.predict(X_test)
Uses the trained model to predict outcomes on the test dataset.

Calculating accuracy:
acc = accuracy_score(y_test, y_pred)
Compares predicted values (y_pred) with actual test labels (y_test) to compute accuracy.

Storing results:
results[name] = acc
Saves each model’s accuracy in the results dictionary.

Displaying results:
print(f"{name} Accuracy: {acc:.4f}")
Prints each model’s accuracy up to 4 decimal places.

# **⚙️ Step 18: Displaying Model Accuracy Scores**

# **🎯 Purpose**

This code is used to print the accuracy percentage of each trained machine learning model on the test dataset.
It provides a clear and concise view of how well each model performs in predicting the target variable (stroke).

# **🧠 Explanation**
# **🔹 Looping Through Trained Models**

for name, model in models.items():


The code loops through each key-value pair in the dictionary models.

name → The name of the model (e.g., "Random Forest", "SVM")

model → The actual trained model object

# **🔹 Evaluating Model Accuracy**

model.score(X_test, y_test)


The .score() method calculates the accuracy of the model.

It compares the model’s predictions on X_test with the true labels y_test.

Returns a value between 0 and 1, representing the proportion of correct predictions.

# **🔹 Formatting Accuracy as Percentage**
"{:.2f}%".format(model.score(X_test, y_test) * 100)


Multiplies accuracy by 100 to convert it into a percentage.

Formats it to two decimal places using :.2f for readability.

Example: 0.9123 → 91.23%

# **🔹 Printing Model Name with Accuracy**
print(name + ": {:.2f}%".format(model.score(X_test, y_test) * 100))


Displays each model’s name followed by its accuracy percentage.

# **Example Output:**

**Logistic Regression: 84.20%**

**Decision Tree: 90.10%**

**Random Forest: 92.45%**

**Gradient Boosting: 93.00%**

**SVM: 87.60%**

**KNN: 88.10%**

# **📊 Step 19: Visualizing Confusion Matrices for All Models**
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict using the best model
y_pred = rf_model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Plot the confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels', fontsize=14)
plt.ylabel('True Labels', fontsize=14)
plt.title('Confusion Matrix - Random Forest', fontsize=16)
plt.show()

"""# **🎯 Purpose**
This code is used to evaluate each model’s classification performance using a Confusion Matrix and visualize it as a heatmap.


It helps you understand where models are making correct predictions and where they are misclassifying data.

# **🧠 Explanation**
# **🔹 Looping Through Each Model**

for name, model in models.items():
Iterates through each trained model in the models dictionary.

name → the model’s name (e.g., "Random Forest")

model → the trained model instance

# **🔹 Making Predictions**

y_pred = model.predict(X_test)
Uses the model to predict labels for the test data (X_test).

Stores the predicted results in y_pred to compare them against the actual labels y_test.

# **🔹 Generating the Confusion Matrix**

cm = confusion_matrix(y_test, y_pred)
The Confusion Matrix (CM) is a 2×2 table (for binary classification) that shows:

True Positives (TP) → Correctly predicted strokes

True Negatives (TN) → Correctly predicted non-strokes

False Positives (FP) → Predicted stroke when it didn’t occur

False Negatives (FN) → Missed predicting an actual stroke

Printed Output Example:

Confusion Matrix
[[950  10]
 [ 35  15]]
# **🔹 Visualizing with Heatmap**


sns.heatmap() → Creates a color-coded matrix visualization:

annot=True → Displays the numeric values inside cells.

fmt='d' → Formats values as integers.

cmap='Blues' → Uses a blue color gradient.

cbar=False → Hides the color bar.

annot_kws={'size':14} → Increases annotation font size.

This makes it easy to visually compare predictions and spot errors.

# **🔹 Adding Labels and Title**
python
Copy code
plt.xlabel('Predicted Labels', fontsize = 14)
plt.ylabel('True Labels', fontsize = 14)
plt.title(f'Confusion Matrix{name}', fontsize = 16)
Labels the X-axis as Predicted Labels (model outputs).

Labels the Y-axis as True Labels (actual outcomes).

Adds a descriptive title for each model (e.g., “Confusion Matrix Random Forest”).

# **🔹 Displaying the Plot**

plt.show()
Renders the confusion matrix heatmap for each model in the loop.

Each model’s matrix is displayed separately.

# **📈 Example Visualization**
Predicted No	Predicted Yes
Actual No	TN = 950	FP = 10
Actual Yes	FN = 35	TP = 15

🔵 Blue areas indicate correct predictions, while lighter shades may indicate misclassifications.

# **📋 Step 20: Generating Classification Reports for Each Model**

# **🎯 Purpose**
This code is used to generate a detailed performance summary of each model using the classification report.

It provides important metrics — precision, recall, F1-score, and accuracy — to help evaluate how well each model predicts stroke occurrences.

# **🧠 Explanation**
# **🔹 Looping Through Models**

for name, model in models.items():
Iterates over all trained models stored in the models dictionary.

name → The model’s name (e.g., "Random Forest", "SVM")

model → The trained model object

# **🔹 Making Predictions**

y_pred = model.predict(X_test)
The model predicts labels (0 or 1) for the test dataset X_test.

These predictions are stored in y_pred for evaluation against the true labels y_test.

# **🔹 Printing Model Name**

print(name + ":")
Displays the name of the current model before showing its performance metrics.

This makes the output organized and easy to read when multiple models are being evaluated.

# **🔹 Generating the Classification Report**

report = classification_report(y_test, y_pred)
The classification_report() function from sklearn.metrics calculates and returns a summary of key performance metrics for classification tasks.

*The report includes:*

Metric	Description
Precision	Of all positive predictions, how many were correct? (TP / (TP + FP))
Recall (Sensitivity)	Of all actual positives, how many did we correctly identify? (TP / (TP + FN))
F1-Score	The harmonic mean of precision and recall — a balanced measure of both.
Support	The number of true instances for each class (0 = no stroke, 1 = stroke).

# **🔹 Printing the Report**

print(report)
Displays the full classification report in tabular format for each model.

Example Output:
Random Forest:
              precision    recall  f1-score   support

           0       0.97      0.99      0.98      965
           1       0.85      0.72      0.78       55

    accuracy                           0.97     1020
   macro avg       0.91      0.86      0.88     1020
weighted avg       0.97      0.97      0.97     1020

# **📈 Step 21: Comparing Model Performance Using Evaluation Metrics and Visualization**
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Create an empty list to store results
model_results = []

# Loop through each trained model
for name, model in models.items():
    y_pred = model.predict(X_test)

    # Calculate performance metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Append the results in a list
    model_results.append([name, acc, prec, rec, f1])

# Convert list to DataFrame
results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

# Print the table
print("\n📊 Model Performance Comparison:")
print(results_df)

# --- Visualization ---
plt.figure(figsize=(10,6))
sns.barplot(data=results_df.melt(id_vars='Model', var_name='Metric', value_name='Score'),
            x='Score', y='Model', hue='Metric', palette='viridis')
plt.title('Model Performance Comparison', fontsize=16)
plt.xlabel('Score')
plt.ylabel('Model')
plt.legend(loc='lower right')
plt.show()

"""# **🎯 Purpose**
This step evaluates all trained models based on multiple performance metrics — accuracy, precision, recall, and F1-score — and then visualizes the comparison using a bar chart.

It helps identify which machine learning algorithm performs best for predicting stroke outcomes.

# **🧠 Explanation**
# **🔹 Importing Metrics**

These functions calculate the main performance metrics used in classification problems:

Accuracy: How many predictions were correct overall.

Precision: How many predicted “stroke” cases were actually strokes.

Recall: How many actual stroke cases were correctly predicted.

F1 Score: The harmonic mean of precision and recall (balances both).

# **🔹 Creating an Empty List for Results**

model_results = []
Initializes an empty list to store performance scores for each model.

Each entry will later contain: [Model Name, Accuracy, Precision, Recall, F1 Score].

# **🔹 Looping Through Each Trained Model**

Iterates through each model in the models dictionary.

Uses each model to predict outcomes on the test data (X_test).

Stores predictions in y_pred.

# **🔹 Calculating Evaluation Metrics**

accuracy_score → Measures overall correctness of predictions.

precision_score → Focuses on how precise the model is in identifying true stroke cases.

recall_score → Measures how well the model detects all actual stroke cases.

f1_score → Balances both precision and recall for fair evaluation.

# **🔹 Storing the Results**

Adds each model’s metrics to the list as a new row.

Example structure:

[
  ['Logistic Regression', 0.84, 0.72, 0.68, 0.70],
  ['Random Forest', 0.92, 0.89, 0.80, 0.84],
  ...
]
# **🔹 Creating a DataFrame for Better Visualization**

Converts the list of results into a pandas DataFrame.

Assigns column names for readability.

This makes it easier to display results in tabular format and plot them.

# **🔹 Printing the Table**

print("\n📊 Model Performance Comparison:")
print(results_df)
Displays a neatly formatted comparison table of all models and their metrics.

Example Output:

Model	Accuracy	Precision	Recall	F1 Score
Logistic Regression	0.84	0.72	0.68	0.70
Random Forest	0.92	0.89	0.80	0.84
Gradient Boosting	0.93	0.88	0.83	0.85

# **🔹 Visualizing the Results**

Explanation:
results_df.melt() → Converts the DataFrame into a format suitable for Seaborn plotting.

sns.barplot() → Creates a grouped bar chart comparing all metrics for each model.

palette='viridis' → Adds a visually appealing color scheme.

Axes & title: Clearly label the plot for interpretation.

## **Summary**

# ***🧠 Stroke Prediction using Machine Learning — Project Summary***
## ***📋 Overview***

This project focuses on predicting the likelihood of a stroke in patients based on health, lifestyle, and demographic data. Using machine learning algorithms, it identifies key risk factors and builds predictive models to support early diagnosis and prevention.

The dataset contains 5,110 patient records with 12 attributes, including age, gender, hypertension, heart disease, BMI, glucose levels, and smoking habits. The target variable stroke indicates whether the patient has had a stroke (1) or not (0).

🧩 Methodology
# **1️⃣ Data Preprocessing**

Loaded the dataset using Pandas and explored its structure.

Handled missing values by imputing the mean BMI value.

Checked for duplicates and data types.

Encoded categorical variables using LabelEncoder.

Applied SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset, as stroke cases were underrepresented.

# **2️⃣ Exploratory Data Analysis (EDA)**

Visualized data distributions using Seaborn and Matplotlib.

Created correlation heatmaps to understand feature relationships.

Identified strong correlations between stroke and factors such as age, hypertension, and glucose levels.

# **3️⃣ Model Development**

Trained and evaluated six machine learning models:

Logistic Regression

Decision Tree

Random Forest 🌟 (Best performer)

Gradient Boosting

Support Vector Machine (SVM)

K-Nearest Neighbors (KNN)

Each model was trained on 80% training data and tested on 20% unseen data using train_test_split with stratification.

# **4️⃣ Model Evaluation**

Models were evaluated using:

Accuracy

Precision

Recall

F1-Score

Confusion Matrix

## **🏆 Results**
Model	Accuracy
Logistic Regression	0.7938
Decision Tree	0.9044
Random Forest	0.9424 ✅
Gradient Boosting	0.8751
SVM	0.5111
KNN	0.8077

The Random Forest Classifier achieved the highest accuracy (94.24%), demonstrating strong performance in predicting stroke occurrence with balanced precision and recall.

## **📈 Key Insights**

Age, hypertension, and glucose level are the most significant predictors of stroke.

Data balancing with SMOTE improved the fairness and reliability of model predictions.

Ensemble models (Random Forest, Gradient Boosting) outperformed individual algorithms due to better generalization.

## **💡 Conclusion**

This project successfully demonstrates how machine learning can assist in stroke risk prediction by analyzing medical and lifestyle factors.
The final Random Forest model provides a reliable framework for early intervention and preventive healthcare decisions.

## **🔮 Future Work**

Implement hyperparameter tuning (GridSearchCV) for optimization.


Deploy as a Streamlit/Flask web app for real-time predictions.


Include additional clinical data (e.g., cholesterol, blood pressure) for improved accuracy.

Enhance model interpretability using SHAP or LIME explanations.
"""

