# -*- coding: utf-8 -*-
"""Medical Insurance Cost Prediction Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14PtiXj2Qz4AYoQmXERp8_SV6xHNcoDfr

# 🏥 **Medical Insurance Cost Prediction Dataset Overview**

---

## 📋 **Dataset Information**

| Detail | Description |
| :------ | :----------- |
| **Dataset Name** | Medical Insurance Cost Dataset |
| **Total Rows** | 1338 |
| **Total Columns** | 7 |
| **Problem Type** | Regression (Supervised Machine Learning) |
| **Objective** | Predict individual **medical insurance charges** based on personal and lifestyle attributes. |

---

## 🎯 **Purpose of the Dataset**

The main purpose of this dataset is to **analyze and predict medical insurance costs**.  
It helps **insurance companies** and **data analysts**:

- Estimate fair premium amounts for individuals.  
- Understand how **health factors** (like BMI or smoking) impact insurance costs.  
- Design **preventive health programs** by identifying risk contributors.  
- Develop **predictive models** that estimate future healthcare expenses.  

This dataset provides a foundation for **data-driven decision-making** in the healthcare and insurance industry.

---

## 🧠 **What Problem Does It Solve?**

| Problem | Explanation |
| :------- | :------------ |
| **Unfair or inaccurate insurance pricing** | Predictive analysis ensures fair pricing based on actual risk factors. |
| **Healthcare cost estimation** | Allows insurance providers to forecast future medical expenses. |
| **Understanding health risk factors** | Helps identify the most influential factors (e.g., smoking, age, obesity). |
| **Data-driven policy decisions** | Enables organizations to make informed choices about coverage and premiums. |

---

## 🧩 **Column Descriptions**

| Column Name | Description | Data Type | Example |
| :------------ | :------------ | :----------- | :--------- |
| **age** | Age of the insured individual (in years). | Numeric | `19`, `35`, `52` |
| **sex** | Gender of the insured individual. | Categorical | `male`, `female` |
| **bmi** | Body Mass Index (weight in kg / height in m²). A measure of body fat. | Numeric (float) | `27.9`, `33.7` |
| **children** | Number of children/dependents covered by the insurance plan. | Numeric (integer) | `0`, `2`, `3` |
| **smoker** | Smoking status of the insured person. | Categorical | `yes`, `no` |
| **region** | Residential region in the United States. | Categorical | `southwest`, `northwest`, `southeast`, `northeast` |
| **charges** | The **medical insurance cost** billed by health insurance. | Numeric (float) | `16884.92`, `1725.55` |

---

## 🔍 **Sample Data (First 5 Rows)**

| age | sex | bmi | children | smoker | region | charges |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 19 | female | 27.9 | 0 | yes | southwest | 16884.92 |
| 18 | male | 33.77 | 1 | no | southeast | 1725.55 |
| 28 | male | 33.00 | 3 | no | southeast | 4449.46 |
| 33 | male | 22.705 | 0 | no | northwest | 21984.47 |
| 32 | male | 28.88 | 0 | no | northwest | 3866.86 |

---

## 🧾 **Summary**

- The dataset includes **1338 records** and **7 columns** related to **demographics**, **lifestyle**, and **medical charges**.  
- The **target variable** is `charges`, representing the **insurance cost**.  
- This dataset is ideal for **regression tasks** such as:
  - Linear Regression  
  - Decision Tree Regressor  
  - Random Forest Regressor  
  - Gradient Boosting Regressor  

---

## 📊 **Key Insights (Optional Section)**

> *(You can include this if you want to make your report more analytical)*

| Observation | Insight |
| :------------ | :--------- |
| **Smokers** | Tend to have significantly higher insurance charges than non-smokers. |
| **Higher BMI** | Positively correlated with increased medical costs. |
| **Older Age** | Older individuals generally face higher insurance charges. |
| **Regional Differences** | Costs vary slightly across regions due to local healthcare rates. |

---

# 🧩 **Library Loading and Preprocessing Overview**
"""

#loading need libraries
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

#preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

"""---

## 🎯 **Purpose**

Before starting any data analysis or machine learning task, we first need to **import important Python libraries**.  
Each of these libraries has a specific role — from handling data and performing mathematical operations to visualizing patterns and preparing data for machine learning.

---
# 🧠 Explanation

## 🔹 Data Analysis & Visualization Libraries

| Library / Module | Purpose / Function |
|------------------|--------------------|
| **NumPy (np)** | Used for numerical computations, mathematical operations, and handling arrays efficiently. |
| **Pandas (pd)** | Helps in loading, cleaning, transforming, and analyzing data in the form of DataFrames. |
| **Seaborn (sns)** | Used for creating advanced and visually appealing statistical plots. |
| **Matplotlib (plt)** | The most common Python library for plotting and visualizing data (line charts, bar graphs, etc.). |
| **SciPy (stats)** | Offers advanced statistical functions for data distribution and hypothesis testing. |

---

## 🔹 Preprocessing Tools

| Function / Class | Purpose |
|------------------|----------|
| **train_test_split** | Splits the dataset into training and testing sets to evaluate model performance. |
| **LabelEncoder** | Converts categorical text data (like ‘male’, ‘female’, ‘yes’, ‘no’) into numeric form since ML models require numbers. |

# **📂 Loading the Dataset**
"""

train = pd.read_csv('/content/insurance.csv')
train

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Importing CSV File** | The function **`pd.read_csv()`** is part of the **Pandas** library and is used to read data from a **CSV (Comma Separated Values)** file. |
| **2. File Path** | The argument **`'/content/train.csv'`** represents the **file path** where the dataset is stored. This path may vary depending on your working environment (e.g., local system, Google Colab, or Jupyter Notebook). |
| **3. Storing in a DataFrame** | The dataset is loaded and stored in a **Pandas DataFrame** named **`train`**. A DataFrame is like a **table structure** with rows and columns, making data easy to analyze and manipulate. |
| **4. Displaying Data** | Simply writing **`train`** displays the **first few rows** of the dataset, allowing us to quickly inspect the data and verify it has been loaded correctly. |

---

## 📘 Why This Step is Important

- It allows us to **load the raw data** into memory for analysis.  
- Acts as the **foundation** for all further preprocessing, visualization, and model training.  
- Without loading the dataset properly, we cannot explore, clean, or train models effectively.  
- It ensures that data is structured in a **DataFrame format**, which simplifies data manipulation and statistical operations.

---

## ⚙️ Example Output (For Understanding)

| age | gender | bmi | children | smoker | region | charges |
|------|---------|------|-----------|----------|----------|----------|
| 19 | female | 27.9 | 0 | yes | southwest | 16884.924 |
| 18 | male | 33.77 | 1 | no | southeast | 1725.552 |
| 28 | male | 33.0 | 3 | no | southeast | 4449.462 |

---

## 🧩 Summary

✅ **`pd.read_csv()`** is the most common method for reading structured data.  
✅ It returns a **DataFrame**, which is the backbone of data analysis in Python.  
✅ Helps us move from **raw dataset → structured format → ready for analysis**.

# **🔍 Checking for Missing Values**
"""

train.isnull().sum()

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `isnull()`** | The function **`isnull()`** checks each cell in the DataFrame and returns **True** if the value is missing (NaN), otherwise **False**. |
| **2. Purpose of `sum()`** | When we apply **`.sum()`**, it counts the total number of **True** values (i.e., missing values) in each column. |
| **3. Combined Use** | Therefore, **`train.isnull().sum()`** gives us the **total number of missing values per column** in the dataset. |

---

## 📊 Output Example

| Column | Missing Values |
|---------|----------------|
| age | 0 |
| gender | 0 |
| bmi | 0 |
| children | 0 |
| smoker | 0 |
| region | 0 |
| charges | 0 |

---

## 🧩 Interpretation

✅ The output shows **0 missing values** in all columns.  
✅ This means our dataset is **clean and complete**, with **no missing entries**.  
✅ Hence, **no data imputation or cleaning** is required at this stage.  

---

## 💡 Why This Step is Important

- Ensures **data quality and integrity** before modeling.  
- Detects **missing or corrupted data** early.  
- Prevents **errors or biases** during machine learning model training.  

✅ **In this case:** *No missing values found — data is ready for preprocessing and exploration!*

# 📏 Understanding `train.shape`
"""

train.shape

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `shape`** | The **`.shape`** attribute in Pandas returns the **dimensions of the DataFrame** as a tuple — `(rows, columns)`. |
| **2. What It Represents** | The first value indicates the **number of rows (data records)**, and the second value indicates the **number of columns (features or attributes)**. |
| **3. How It Helps** | It helps us understand the **size of the dataset**, which is crucial for planning data analysis, visualization, and model training. |

---

## 📊 Example Output

```python
(1338, 7)
```

## 🧩 Interpretation

| Element | Meaning |
|----------|----------|
| **1338** | Total number of rows — represents the total number of individuals or records in the dataset. |
| **7** | Total number of columns — represents the different features or attributes (like age, gender, bmi, etc.). |

---

## 💡 Why This Step is Important

- Helps in **understanding dataset size and structure**.  
- Useful for **data validation** and ensuring the dataset loaded correctly.  
- Provides quick insight into **how much data** we have for analysis and model training.  

✅ **In this case:** The dataset has **1338 rows** and **7 columns**, which makes it **suitable for building a predictive model.**

# ℹ️ Understanding `train.info()`
"""

train.info()

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `info()`** | The **`.info()`** function in Pandas provides a **concise summary** of the DataFrame. It helps you understand the **structure and composition** of your dataset. |
| **2. What It Displays** | It shows the **number of entries (rows)**, **column names**, **data types (dtypes)**, **non-null counts**, and **memory usage**. |
| **3. Why It’s Useful** | It gives a **quick overview** to check if there are **missing values**, identify **data types** (like int, float, object), and estimate how much **memory** the dataset consumes. |

---

## 🧩 Interpretation

| Information | Meaning |
|--------------|----------|
| **Entries (1338)** | Total number of records (rows) in the dataset. |
| **Columns (7)** | Number of features or attributes in the dataset. |
| **Non-null Count** | Shows there are **no missing values** in any column. |
| **Dtype** | Indicates the **data type** of each column (int64, float64, object). |
| **Memory Usage** | Shows the **amount of RAM** used by the DataFrame (approx. 73.3 KB). |

---

## 💡 Why This Step is Important

- Provides a **quick summary** of the dataset.  
- Helps in **detecting missing data** and **incorrect data types** early.  
- Useful for **memory optimization** and planning **data preprocessing**.  

✅ **In this case:** The dataset is **complete**, **well-structured**, and **ready for analysis**.

# **🧹 checking and Removing Duplicate Records**
"""

train.duplicated().sum()
train.drop_duplicates(inplace=True)

"""# 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `duplicated()`** | The `duplicated()` function checks for **duplicate rows** in the DataFrame. It returns **True** for each row that is a duplicate and **False** otherwise. |
| **2. Using `sum()`** | When we apply `.sum()`, it counts the total number of **duplicate rows** present in the dataset. |
| **3. Purpose of `drop_duplicates()`** | The `drop_duplicates()` function removes **all duplicate rows** from the DataFrame, keeping only the **first occurrence** of each unique record. |
| **4. `inplace=True`** | This parameter updates the existing DataFrame **in place**, meaning the changes are applied directly to the current dataset without creating a copy. |

---

# 📊 Example Output

```python
train.duplicated().sum()
# Output: 1

✅ There was **1 duplicate row** in the dataset.

After running:

```python
train.drop_duplicates(inplace=True)
```
**The duplicate record was successfully removed.**

# 🧠 Selecting Integer Columns
"""

int_col = train.select_dtypes(include=['int64']).columns.tolist()
train[int_col]

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of select_dtypes()** | The `select_dtypes()` function in Pandas is used to select columns in a DataFrame based on their data type. |
| **2. include=['int64']** | This parameter filters columns that have the data type **int64** (i.e., integer values). |
| **3. .columns.tolist()** | Extracts only the column names (not data) and converts them into a **Python list** for easy access. |
| **4. train[int_col]** | Displays only those columns from the dataset that contain **integer data**. |

---

## 📊 Example Output

| Column Name | Description |
|--------------|--------------|
| age | Represents the age of the individual. |
| children | Number of children/dependents. |
| region_code | Encoded region or location identifier. |

*(Example columns may vary depending on dataset structure.)*

---

## 🧩 Interpretation

| Element | Meaning |
|----------|----------|
| **int_col** | A list containing names of all integer-type columns. |
| **train[int_col]** | Displays a filtered DataFrame showing only integer columns. |

---

## 💡 Why This Step is Important

- Helps analyze **numeric columns** separately for statistical or model-preparation purposes.  
- Useful for **feature selection** when only integer variables are needed.  
- Makes it easier to apply **transformations or visualizations** specific to numeric data types.  
- Supports **data cleaning** and ensures proper data-type handling before model training.  

✅ **In this case:** The code extracts all integer columns from the dataset, allowing focused analysis or preprocessing on only numerical features.

"""

float_col = train.select_dtypes(include=['float64']).columns.tolist()
train[float_col]

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of select_dtypes()** | The `select_dtypes()` function in Pandas is used to select columns in a DataFrame based on their data type. |
| **2. include=['float64']** | This parameter filters columns that have the data type **float64** (i.e., decimal or continuous numeric values). |
| **3. .columns.tolist()** | Extracts only the column names (not the data) and converts them into a **Python list** for easy access and reference. |
| **4. train[float_col]** | Displays only those columns from the dataset that contain **floating-point (decimal)** data. |

---

## 📊 Example Output

| Column Name | Description |
|--------------|--------------|
| bmi | Represents the body mass index (weight-to-height ratio). |
| charges | The medical insurance cost billed to the patient. |

*(Example columns may vary depending on dataset structure.)*

---

## 🧩 Interpretation

| Element | Meaning |
|----------|----------|
| **float_col** | A list containing names of all float-type (decimal) columns. |
| **train[float_col]** | Displays a filtered DataFrame showing only float columns. |

---

## 💡 Why This Step is Important

- Helps focus on **continuous numeric data**, which is essential for statistical analysis and regression modeling.  
- Useful for performing **scaling, normalization, or correlation** analysis on float-type columns.  
- Makes it easier to detect patterns or outliers in continuous features.  
- Ensures that all numerical attributes are handled appropriately before model training.  

✅ **In this case:** The code extracts all floating-point columns from the dataset, allowing targeted analysis and preprocessing of continuous numerical features.

"""

cat_col = train.select_dtypes(include=['object']).columns.tolist()
train[cat_col]

"""## 🔹 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of select_dtypes()** | The `select_dtypes()` function in Pandas is used to select columns in a DataFrame based on their data type. |
| **2. include=['object']** | This parameter filters columns that have the **object** data type — usually containing text, strings, or categorical data. |
| **3. .columns.tolist()** | Extracts only the column names (not the data) and converts them into a **Python list** for easy access. |
| **4. train[cat_col]** | Displays only those columns from the dataset that contain **categorical (non-numeric)** data. |

---

## 📊 Example Output

| Column Name | Description |
|--------------|--------------|
| gender | Indicates whether the person is male, female, or other. |
| smoker | Specifies whether the person is a smoker or not. |
| region | Represents the geographical region of the individual (e.g., northeast, southeast, southwest, northwest). |

*(Example columns may vary depending on dataset structure.)*

---

## 🧩 Interpretation

| Element | Meaning |
|----------|----------|
| **cat_col** | A list containing names of all categorical (object-type) columns. |
| **train[cat_col]** | Displays a filtered DataFrame showing only categorical columns. |

---

## 💡 Why This Step is Important

- Helps identify **non-numeric** (categorical) features that need **encoding** before feeding into machine learning models.  
- Enables separate **analysis of categorical variables**, such as frequency counts or relationships with the target variable.  
- Simplifies **feature engineering** by focusing on columns that require transformation (e.g., one-hot encoding or label encoding).  
- Ensures proper **data type handling** for accurate modeling and visualization.  

✅ **In this case:** The code extracts all categorical columns (object type) from the dataset, enabling focused preprocessing on text-based or categorical features before model training.

## 🧩 Checking for Data Type Inconsistencies

Sometimes, columns that should contain **categorical data** (like gender, region, smoker) are stored as **numeric (int or float)** — or vice versa.  
Such inconsistencies can cause problems in encoding, visualization, or model training.

---

## 🔹 Step-by-Step Code
"""

# Check the data types of all columns
train.dtypes

# Identify numeric columns that might contain categorical data
for col in train.columns:
    unique_values = train[col].nunique()
    if train[col].dtype in ['int64', 'float64'] and unique_values < 10:
        print(f"⚠️ Possible Categorical Column (Stored as Numeric): {col} | Unique Values: {unique_values}")

# Identify object columns that might contain numeric data
for col in train.select_dtypes(include=['object']).columns:
    if train[col].str.isnumeric().any():
        print(f"⚠️ Possible Numeric Column (Stored as Object): {col}")

"""---

## 🧠 Interpretation

| Element | Meaning |
|----------|----------|
| **Column:** `children` | The column detected with potential inconsistency. |
| **Unique Values:** `6` | Indicates that this column contains only 6 distinct numbers. |
| **Type:** Numeric (`int64`) | Currently stored as an integer data type. |

---

## 🔍 What This Means

- The column **`children`** has **only 6 unique integer values** (e.g., 0, 1, 2, 3, 4, 5).  
- Even though it’s numeric, it represents **categories** — i.e., the **number of children** a person has.  
- This type of variable is **discrete** (not continuous), and while it’s numeric, in modeling terms, it often behaves like a **categorical feature**.

---

## 💡 Why It’s Important

- Machine Learning models may treat `children` as a **continuous numerical variable**, which could slightly affect predictions if not handled properly.  
- However, in most cases, leaving it as **integer** is acceptable because the number of children has a natural numeric order (0 < 1 < 2 < …).  
- But if the column represents **groups without order**, then you should convert it to a categorical type.

---

## ✅ What You Can Do

| Situation | Recommended Action |
|------------|--------------------|
| If `children` is used for **numerical analysis** | Keep it as `int64` (no issue). |
| If `children` is used for **categorical grouping (e.g., visualization)** | Convert it to categorical:  |
|  | ```python
train['children'] = train['children'].astype('object')
``` |

---

### 🧩 Summary

- ✅ There is **no serious inconsistency**, just a **data type consideration**.  
- The `children` column is **numerical but discrete** — it can be treated as **either numeric or categorical**, depending on your analysis needs.  
- For model training, you can **keep it numeric**.  
- For exploratory analysis (like bar charts), you may convert it to **categorical** for clearer visual interpretation.

## 📊 Visualizing Target Variable Distribution
"""

from scipy import stats

plt.subplots(figsize=(9,9))
sns.distplot(train['charges'], fit=stats.norm)

(mu, sigma) = stats.norm.fit(train['charges'])

# plot with the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

"""## 🎯 Target Column Identification

In this dataset, we are predicting **medical insurance cost**, so the **target column** is:

### 🏷️ `charges`

---

## 🧠 Explanation

| Column | Description |
|---------|--------------|
| **charges** | Represents the **medical insurance cost** billed to the patient. It is a **continuous numeric (float)** variable. |

This is the **dependent variable (Y)** —  
it’s what we want our machine learning model to **predict** based on other features like age, gender, BMI, smoker status, etc.

---

## 📊 Example

| age | gender | bmi | children | smoker | region | charges |
|-----|---------|-----|-----------|---------|---------|----------|
| 19  | female | 27.9 | 0 | yes | southwest | 16884.92 |
| 35  | male | 22.3 | 1 | no | southeast | 3587.45 |

Here, all columns **except `charges`** are **input features (independent variables)**,  
and **`charges`** is the **output (target variable)**.

---

## 💡 Why This Column is the Target

- It’s the **main value to be predicted** in a **regression problem**.  
- The goal of the project is to **estimate or predict medical insurance cost** based on patient details.  
- Hence, `charges` is the **target variable (Y)**,  
and all other columns (`age`, `gender`, `bmi`, `children`, `smoker`, `region`) are **independent variables (X)**.

---

✅ **Final Answer:**  
> 🎯 **Target Column:** `charges`  
> 📈 **Type:** Continuous numeric (float)  
> 🧩 **Prediction Goal:** Predicting medical insurance cost based on other features.


"""

train['charges']

"""# 📊 Distribution Analysis of Target Variable (`charges`)

## 🧠 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. from scipy import stats** | Imports the `stats` module from **SciPy**, which provides powerful statistical functions such as fitting data to distributions. |
| **2. plt.subplots(figsize=(12,9))** | Creates a new plotting figure with a defined size (12x9 inches) for better visibility of the chart. |
| **3. sns.distplot(train['charges'], fit=stats.norm)** | Draws a **histogram** of the `charges` column and overlays a **Normal Distribution curve** (`stats.norm`) on top of it. This helps visualize how closely the data follows a normal distribution. |
| **4. (mu, sigma) = stats.norm.fit(train['charges'])** | Calculates and returns the two key parameters of the normal distribution: <br> **μ (mean)** → average value of `charges` <br> **σ (standard deviation)** → how much the `charges` values spread out from the mean. |
| **5. plt.legend(...)** | Displays a **legend** on the chart, showing the calculated mean (μ) and standard deviation (σ) values for quick reference. |
| **6. plt.ylabel('Frequency')** | Labels the Y-axis as “Frequency”, indicating how often specific charge values occur in the dataset. |

---

## 📘 Example Output

The output will show a **bell-shaped curve** (if the data follows a normal distribution).

🧾 Example Legend:



```

Normal dist. (μ = 13270.42 and σ = 12110.56)
```


---

## 🧩 Interpretation

| Element | Meaning |
|----------|----------|
| **μ (Mu)** | Represents the **mean** of the `charges` — the average cost of insurance. |
| **σ (Sigma)** | Represents the **standard deviation**, which tells how widely the insurance charges are spread from the mean. |
| **Curve Shape** | If the curve is **skewed** (not symmetric), it means the data is **not normally distributed**, which is **common in medical cost data** because some people have very high charges due to severe conditions. |

---

## 💡 Why This Step is Important

- Helps understand how **insurance charges** are spread among individuals.  
- Identifies whether the **target variable (`charges`)** follows a **normal or skewed** distribution.  
- Useful for deciding whether **data transformation** (like `log` or `sqrt`) is needed before building regression models.  
- Helps **detect outliers** and **understand variance** in target values, which affects model performance.

---

## ✅ Conclusion

This visualization step:
- Confirms the **shape** and **spread** of the target variable.  
- Provides insights into **data normality** and **variability**.  
- Is an essential **exploratory data analysis (EDA)** step before applying any machine learning model.  

✅ **The code is correct** and gives meaningful statistical insights into the `charges` column.

"""

#we use log function which is in numpy
train['charges'] = np.log1p(train['charges'])

#Check again for more normal distribution
plt.subplots(figsize=(12,9))
sns.distplot(train['charges'], fit=stats.norm)

# Get the fitted parameters used by the function
(mu, sigma) = stats.norm.fit(train['charges'])

# plot with the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

"""# ⚙️ Log Transformation for Normal Distribution

## 🔹 Code Explanation

### Step-by-Step Description

| Step | Description |
|------|--------------|
| 1. `train['charges'] = np.log1p(train['charges'])` | Applies **log transformation** to the `charges` column using NumPy’s `log1p()` function. It computes `log(1 + x)` which is safer than `log(x)` because it handles **zero values** without causing errors. |
| 2. `plt.subplots(figsize=(12,9))` | Creates a large plotting area for clear visualization. |
| 3. `sns.distplot(train['charges'], fit=stats.norm)` | Plots the **histogram** of the log-transformed `charges` column and overlays a **normal distribution curve** fitted to the data. |
| 4. `(mu, sigma) = stats.norm.fit(train['charges'])` | Calculates the **mean (μ)** and **standard deviation (σ)** of the transformed data. |
| 5. `plt.legend([...])` | Displays the calculated μ and σ values on the graph as a legend. |
| 6. `plt.ylabel('Frequency')` | Labels the Y-axis to show how frequently each transformed value appears. |

---

## 📊 Example Output
After applying the log transformation,  
the distribution becomes **more symmetric** and **closer to a normal bell curve**.

Example Legend:
Normal dist. (μ = 9.13 and σ = 0.69)


---

## 🧠 Interpretation

| Element | Meaning |
|----------|----------|
| `μ (Mu)` | Mean (average) of the log-transformed charges. |
| `σ (Sigma)` | Spread or variability of the transformed values. |
| Shape | Should now appear **more normally distributed** (less skewed). |

---

## 💡 Why This Step is Important

- **Reduces right skewness** in the target variable (`charges`).
- Makes the data **closer to a normal distribution**, which improves model performance for algorithms assuming normality (like Linear Regression).
- **Stabilizes variance** and reduces the impact of extreme outliers.
- Ensures **better statistical analysis** and model interpretation.

---

## ✅ In This Case:
The original `charges` variable was **right-skewed**.  
After applying `np.log1p()`, the data distribution became **more normal**,  
making it suitable for modeling and predictive analysis.

# **Boxplot to Detect Outliers**
"""

plt.figure(figsize=(10,5))
sns.boxplot(x=train['charges'], color='lightcoral')
plt.title('Boxplot of Charges', fontsize=16)
plt.xlabel('Charges')
plt.show()

"""# **3️⃣ Violin Plot by Gender**"""

plt.figure(figsize=(8,6))
sns.violinplot(x='sex', y='charges', data=train, palette='Set2')
plt.title('Distribution of Charges by Gender', fontsize=16)
plt.show()

"""**📘 Interpretation**

Shows how charges differ between male and female.
Combines boxplot + KDE — so you can see both range and density.

# Scatter Plot: Age vs Charges
"""

plt.figure(figsize=(10,6))
sns.scatterplot(x='age', y='charges', hue='smoker', data=train, palette='coolwarm', alpha=0.7)
plt.title('Age vs Charges (Colored by Smoker Status)', fontsize=16)
plt.show()

"""**📘 Interpretation**

Shows the relationship between age and charges.
You’ll notice smokers (usually in red) have much higher medical costs than non-smokers.

# Pairplot for Numeric Features
"""

numeric_cols = train.select_dtypes(include=['int64','float64']).columns
sns.pairplot(train[numeric_cols], diag_kind='kde', corner=True)
plt.suptitle('Pairwise Relationships Among Numeric Features', y=1.02, fontsize=16)
plt.show()

"""**📘 Interpretation**

Shows all numeric features plotted against each other.
Diagonal plots show individual distributions, while others show correlations and trends.

## 💡 Why These Visuals Matter

✅ **Histogram** — Checks data symmetry or skewness  
✅ **Boxplot** — Detects outliers and spread  
✅ **Violin Plot** — Compares distributions across categories  
✅ **Scatter Plot** — Shows feature relationships and smoking impact  
✅ **Pairplot** — Gives a holistic view of all numeric features  

---

## 🎯 Summary

These five visuals give you:

- 🧠 Strong EDA foundation  
- 💰 Insight into target variable (**charges**) behavior  
- 🔍 Better understanding of variable relationships  
- 🧩 Clarity for feature engineering and model selection
"""

train_corr = train.select_dtypes(include=[np.number])
train_corr.shape

from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize label encoder
le = LabelEncoder()

# Encode all categorical columns directly in the original DataFrame
for col in train.select_dtypes(include=['object']).columns:
    train[col] = le.fit_transform(train[col])

# Now calculate correlation
corr = train.corr()

# Plot the heatmap
plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap (After Label Encoding)', fontsize=14)
plt.show()

"""## 🧠 Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. train.corr()** | Calculates the correlation matrix between all numeric columns in the dataset. |
| **2. sns.heatmap()** | Creates a visual heatmap to represent correlation strength and direction between variables. |
| **3. annot=True** | Displays the correlation values on each cell for clarity. |
| **4. cmap='coolwarm'** | Uses a color gradient (blue → red) to represent negative and positive correlations. |
| **5. fmt=".2f"** | Formats the correlation values to two decimal places. |
| **6. linewidths=0.5** | Adds thin white lines between cells for better readability. |
| **7. plt.title()** | Adds a descriptive title to the heatmap. |

---

## 🧩 Interpretation

| Correlation Range | Meaning |
|--------------------|---------|
| **+1.0** | Perfect positive correlation — when one increases, the other also increases. |
| **0.0** | No correlation — no linear relationship between variables. |
| **-1.0** | Perfect negative correlation — when one increases, the other decreases. |

---

## 💡 Why This Step is Important

- Helps identify **strong relationships** between features (e.g., age vs. charges).  
- Useful for **feature selection** — keeping variables that contribute to prediction.  
- Detects **multicollinearity**, where two or more variables are highly correlated.  
- Provides a quick **visual summary** of data relationships.  

✅ **In this dataset:** You’ll likely observe that `smoker` and `charges` have a **strong positive correlation**, meaning smoking greatly increases medical costs.

# 📊 Correlation Analysis — Medical Insurance Dataset

## 🔍 Heatmap Insights

| Feature Pair | Correlation (r) | Relationship Strength | Interpretation |
|---------------|----------------|------------------------|----------------|
| **smoker ↔ charges** | **0.67** | 🟢 **Strong Positive** | Smoking greatly increases insurance charges. Smokers pay much higher medical costs. |
| **age ↔ charges** | **0.53** | 🟡 **Moderate Positive** | Older individuals tend to have higher medical charges. Age impacts cost significantly. |
| **bmi ↔ charges** | **0.13** | 🔵 **Weak Positive** | Higher BMI (body mass index) slightly increases insurance costs but not very strongly. |
| **children ↔ charges** | **0.16** | 🔵 **Weak Positive** | Number of children doesn’t affect medical charges much. |
| **sex ↔ charges** | **0.01** | ⚪ **No Correlation** | Gender has almost no influence on medical charges. |
| **region ↔ charges** | **-0.04** | ⚪ **No Correlation** | Region does not significantly affect insurance charges. |

---

## 🧠 Key Takeaways

- Smoking is the most influential factor affecting medical charges.  
- Age also plays a strong role — costs increase with age.  
- BMI and number of children have minor effects.  
- Sex and region are practically irrelevant in determining insurance cost.  

---

## 📈 Conclusion

- Features like **smoker** and **age** are **highly important** for prediction models.  
- You can **focus feature engineering** on these two for better model accuracy.  
- Weakly correlated columns (**sex**, **region**) may contribute **little** to the target variable.
"""

print("Find most important features relative to target")
corr = train_corr.corr()
corr.sort_values(['charges'], ascending=False, inplace=True)
corr.charges

"""# **Preparing the Data for Modeling**"""

y = train['charges']
X = train.drop('charges', axis = 1)

"""# 🧠 Train-Test Split — Medical Insurance Dataset"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""## 🔹 Purpose

**Why we do this:**  
We split the dataset into **training** and **testing** sets to evaluate the performance of a machine learning model on **unseen data**.

- **Training set (`X_train`, `y_train`)**: Used to train the model. The model **learns patterns** from this data.  
- **Testing set (`X_test`, `y_test`)**: Used to **test the model’s performance** on data it has never seen.

**`test_size=0.2`**  
- This means **20% of the data** is used for testing, and **80% for training**.  
- This is a common ratio that balances **learning** and **evaluation**.

## 🔹 Outcome

- The model is trained on `X_train` and `y_train`.  
- After training, we **predict on `X_test`** and compare predictions with `y_test`.  
- This helps to check if the model **generalizes well** and avoids **overfitting** (memorizing training data instead of learning patterns).

## ✅ Key Points

- Always split data **before training**.  
- Random splitting ensures the model sees a **representative sample** of data in both sets.  
- `train_test_split` **shuffles data by default**, which helps in better randomness.
"""

X_test

y_test

"""# **🗂️ Data Preparation: Dropping Unnecessary Columns**"""

train_copy = train.drop(columns=['sex', 'region'])

"""**✅ Explanation**

Created a copy of the original dataframe to avoid modifying it directly.

Dropped columns sex and region because their correlation with the target charges is negligible.

train_copy will now be used for model training and evaluation.

# **📊 Gradient Boosting Regressor: Model Evaluation**
"""

# 📌 Step 2: Separate features and target
X = train_copy.drop(columns=['charges'])
y = train_copy['charges']

# 📌 Step 3: Split into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 📌 Step 4: Encode categorical feature 'smoker'
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X_train['smoker'] = le.fit_transform(X_train['smoker'])
X_test['smoker'] = le.transform(X_test['smoker'])

# 📌 Step 5: Import Gradient Boosting & Metrics
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize model
gbr = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)

# Train the model
gbr.fit(X_train, y_train)

# Predict on test set
y_pred = gbr.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Prediction for a single row (142th row)
pred_single = gbr.predict(X_test.iloc[[142]])[0]
real_single = y_test.iloc[142]

# Print results
print("--- Gradient Boosting Regressor ---")
print("Accuracy (R² Score)      :", r2*100)
print("Mean Absolute Error       :", mae)
print("Mean Squared Error        :", mse)
print("Root Mean Squared Error   :", rmse)
print(f"Prediction (142th row)   : {pred_single}")
print(f"Real Value (142th row)   : {real_single}")

"""---

## 🧠 Overview
**Gradient Boosting Regressor (GBR)** ek ensemble learning technique hai jo **multiple weak learners (decision trees)** ko sequentially combine karke ek **strong predictive model** banata hai.  
- Har naya tree **pehle ke errors ko reduce** karne ki koshish karta hai.  
- Popular for **regression problems** jahan target continuous hota hai.  
- Advantages:
  - High accuracy
  - Handles non-linear relationships well
  - Reduces overfitting compared to a single decision tree

---

## 🔹 Model Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Accuracy (R² Score) | 85.85% | Model explains ~86% of variance in insurance charges. High predictive power. |
| Mean Absolute Error (MAE) | 0.2059 | On average, prediction is off by ~0.206 units (log scale/normalized). |
| Mean Squared Error (MSE) | 0.1313 | Squared error penalizes larger mistakes more than MAE. |
| Root Mean Squared Error (RMSE) | 0.3624 | Standard deviation of prediction errors. Lower is better. |

---

## 🔹 Single Row Prediction Example

| Row | Predicted Value | Real Value | Interpretation |
|-----|----------------|------------|----------------|
| 142 | 10.627 | 10.733 | Model prediction is very close to actual value, indicating good performance. |

---

## 📌 Key Takeaways
- GBR is the **best performer** among the tested models for this dataset.  
- Features like `smoker` and `age` contribute most to prediction accuracy.  
- Weakly correlated features (`children`, `bmi`) slightly affect the target but model handles them well.  
- Model can be used confidently for predicting **insurance charges** in similar datasets.

# 📝 Final Summary: Insurance Charges Prediction

---

## 🧠 Problem Statement
The dataset contains **medical insurance data** for 1338 individuals with features such as:
- Age
- Sex
- BMI
- Number of children
- Smoking status
- Region
- Charges (target)

The goal was to **predict insurance charges** (`charges`) based on personal and medical features.

---

## 🔹 Data Preparation
- Dropped **unnecessary columns**: `sex`, `region` (correlation with charges negligible).  
- Encoded **categorical feature** `smoker` to numeric for modeling.  
- Split dataset into **training and test sets** (80%-20%).

---

## 🔹 Model Training & Selection
- Multiple regression models tested:  
  - Linear Regression  
  - Decision Tree Regressor  
  - Random Forest Regressor  
  - Gradient Boosting Regressor  
  - Support Vector Regressor  

- **Gradient Boosting Regressor** selected as the **best performer**:
  - R² Score: **85.85%**  
  - MAE: **0.206**  
  - RMSE: **0.362**  
  - Prediction for a sample row very close to actual charges.

---

## 🔹 Key Findings
- **Smoking status** and **age** are the most influential factors affecting insurance charges.  
- **BMI** and **number of children** have minor influence.  
- **Sex** and **region** have negligible effect.  
- Gradient Boosting Regressor efficiently predicts insurance costs with high accuracy.

---

## ✅ Conclusion
- Problem of predicting medical insurance charges was successfully solved using **Gradient Boosting Regressor**.  
- The model can be used for **accurate estimation** of insurance costs for new individuals based on relevant features.  
- Feature selection (dropping low-correlation columns) improved model focus and performance.

---
"""